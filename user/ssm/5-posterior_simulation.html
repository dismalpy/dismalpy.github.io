

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Posterior Simulation &mdash; DismalPy 0.2.1 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="author" title="About these documents"
              href="../../about.html"/>
    <link rel="top" title="DismalPy 0.2.1 documentation" href="../../index.html"/>
        <link rel="up" title="State Space Models" href="../ssm.html"/>
        <link rel="next" title="Out-of-the-box models" href="6-out-of-the-box_models.html"/>
        <link rel="prev" title="Maximum Likelihood Estimation" href="4-maximum_likelihood_estimation.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        

        
          <a href="../../contents.html" class="icon icon-home"> DismalPy
        

        
        </a>

        
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

        
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
          
          
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../examples.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../_notebooks/sarimax_internet.ipynb.html">Durbin and Koopman: Box-Jenkins Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_notebooks/sarimax_stata.ipynb.html">SARIMAX: Introduction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/sarimax_stata.ipynb.html#arima-example-1-arima">ARIMA Example 1: Arima</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/sarimax_stata.ipynb.html#arima-example-2-arima-with-additive-seasonal-effects">ARIMA Example 2: Arima with additive seasonal effects</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/sarimax_stata.ipynb.html#arima-example-3-airline-model">ARIMA Example 3: Airline Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/sarimax_stata.ipynb.html#arima-example-4-armax-friedman">ARIMA Example 4: ARMAX (Friedman)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/sarimax_stata.ipynb.html#arima-postestimation-example-1-dynamic-forecasting">ARIMA Postestimation: Example 1 - Dynamic Forecasting</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../_notebooks/local_linear_trend.ipynb.html">State space modeling: Local Linear Trends</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/local_linear_trend.ipynb.html#references">References</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../_notebooks/structural_harvey_jaeger.ipynb.html">Detrending, Stylized Facts and the Business Cycle</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/structural_harvey_jaeger.ipynb.html#unobserved-components">Unobserved Components</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../_notebooks/structural_harvey_jaeger.ipynb.html#trend">Trend</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_notebooks/structural_harvey_jaeger.ipynb.html#seasonal">Seasonal</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_notebooks/structural_harvey_jaeger.ipynb.html#cycle">Cycle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_notebooks/structural_harvey_jaeger.ipynb.html#irregular">Irregular</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_notebooks/structural_harvey_jaeger.ipynb.html#regression-effects">Regression effects</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/structural_harvey_jaeger.ipynb.html#data">Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/structural_harvey_jaeger.ipynb.html#model">Model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../_notebooks/dfm_coincident.ipynb.html">Dynamic factors and coincident indices</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/dfm_coincident.ipynb.html#macroeconomic-data">Macroeconomic data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/dfm_coincident.ipynb.html#dynamic-factors">Dynamic factors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/dfm_coincident.ipynb.html#model-specification">Model specification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/dfm_coincident.ipynb.html#parameter-estimation">Parameter estimation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/dfm_coincident.ipynb.html#estimates">Estimates</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../_notebooks/dfm_coincident.ipynb.html#parameters">Parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_notebooks/dfm_coincident.ipynb.html#estimated-factors">Estimated factors</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/dfm_coincident.ipynb.html#post-estimation">Post-estimation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/dfm_coincident.ipynb.html#coincident-index">Coincident Index</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/dfm_coincident.ipynb.html#appendix-1-extending-the-dynamic-factor-model">Appendix 1: Extending the dynamic factor model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../_notebooks/varmax.ipynb.html">VARMAX models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/varmax.ipynb.html#model-specification">Model specification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/varmax.ipynb.html#example-1-var">Example 1: VAR</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/varmax.ipynb.html#example-2-vma">Example 2: VMA</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_notebooks/varmax.ipynb.html#caution-varma-p-q-specifications">Caution: VARMA(p,q) specifications</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">DismalPy User Guide</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../ssm.html">State Space Models</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="../ssm.html#topics">Topics</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="2-state_space_models.html">State space models</a></li>
<li class="toctree-l4"><a class="reference internal" href="3-python_representation.html">Representation in Python</a></li>
<li class="toctree-l4"><a class="reference internal" href="4-maximum_likelihood_estimation.html">Maximum Likelihood Estimation</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="">Posterior Simulation</a></li>
<li class="toctree-l4"><a class="reference internal" href="6-out-of-the-box_models.html">Out-of-the-box models</a></li>
<li class="toctree-l4"><a class="reference internal" href="9-references.html">References</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../ssm.html#examples">Examples</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../_notebooks/sarimax_internet.ipynb.html">Durbin and Koopman: Box-Jenkins Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_notebooks/sarimax_stata.ipynb.html">SARIMAX: Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_notebooks/local_linear_trend.ipynb.html">State space modeling: Local Linear Trends</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_notebooks/structural_harvey_jaeger.ipynb.html">Detrending, Stylized Facts and the Business Cycle</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_notebooks/dfm_coincident.ipynb.html">Dynamic factors and coincident indices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_notebooks/varmax.ipynb.html">VARMAX models</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/index.html">DismalPy Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../reference/ssm.html">State Space Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../reference/ssm.html#built-in-models">Built-in models</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../reference/ssm/sarimax.html">SARIMAX</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../reference/ssm/structural.html">Unobserved Components</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../reference/ssm/varmax.html">VARMAX</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../reference/ssm/dynamic_factor.html">Dynamic Factors</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../reference/ssm.html#extension-starting-point">Extension starting point</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../reference/ssm/mlemodel.html">MLEModel</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../reference/ssm.html#base-classes">Base classes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../reference/ssm/representation.html">Representation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../reference/ssm/kalman_filter.html">Kalman filter</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../reference/ssm/kalman_smoother.html">Kalman smoother</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../reference/ssm/simulation_smoother.html">Simulation Smoother</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../reference/ssm/model.html">Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../reference/ssm/tools.html">Tools</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#dependencies">Dependencies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#procedure">Procedure</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../installation.html#installing-from-source">Installing from source</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../release.html">Release Notes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../release.html#dismalpy-0-2-0-release-notes">DismalPy 0.2.0 Release Notes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../release.html#highlights">Highlights</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../release.html#dismalpy-0-1-0-release-notes">DismalPy 0.1.0 Release Notes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../release.html#id1">Highlights</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../release.html#dropped-support">Dropped Support</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../release.html#future-changes">Future Changes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../release.html#compatibility-notes">Compatibility notes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../release.html#new-features">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../release.html#improvements">Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../release.html#changes">Changes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../release.html#deprecations">Deprecations</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../about.html">About DismalPy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about.html#about-this-documentation">About this documentation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../about.html#conventions">Conventions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../bugs.html">Reporting bugs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../license.html">DismalPy License</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">Glossary</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../glossary.html#module-dismalpy.doc.jargon">Jargon</a></li>
</ul>
</li>
</ul>

          
        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../contents.html">DismalPy</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../contents.html">Docs</a> &raquo;</li>
      
          <li><a href="../index.html">DismalPy User Guide</a> &raquo;</li>
      
          <li><a href="../ssm.html">State Space Models</a> &raquo;</li>
      
    <li>Posterior Simulation</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/user/ssm/5-posterior_simulation.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document">
            
  <div class="section" id="posterior-simulation">
<h1>Posterior Simulation<a class="headerlink" href="#posterior-simulation" title="Permalink to this headline">¶</a></h1>
<p>State space models are also amenable to parameter estimation by Bayesian
methods. We consider posterior simulation by Markov chain Monte Carlo (MCMC)
methods, and in particular using the Metropolis-Hastings and Gibbs sampling
algorithms. This section describes how to use the above models in Bayesian
estimation, but fortunately no further modifications need be made. Classes
defined as in the maximum likelihood section (i.e. classes that extend from
<code class="docutils literal"><span class="pre">dp.ssm.MLEModel</span></code>) can be used for either maximum likelihood estimation
or Bayesian estimation. Thus the example code is only tasked with <em>applying</em>
the previously defined state space models.</p>
<p>A full discussion of Bayesian techniques is beyond the scope of this paper, but
interested readers can consult <a class="reference internal" href="9-references.html#koop-bayesian-2003" id="id1">[17]</a> for a general
introduction to Bayesian econometrics, <a class="reference internal" href="9-references.html#west-bayesian-1999" id="id2">[32]</a> for a
comprehensive Bayesian approach to state space models, and
<a class="reference internal" href="9-references.html#kim-state-space-1999" id="id3">[15]</a> for a excellent practical text on parameter
estimation in state space models. The following introduction to Bayesian
methods is drawn from these references.</p>
<p>The Bayesian approach to parameter estimation begins by considering parameters
as random variables. Bayes&#8217; theorem is applied to derive a distribution for the
parameters conditional on the observed data. This &#8220;posterior&#8221; distribution is
proportional to the likelihood function multiplied by a &#8220;prior&#8221; distribution
for the parameters. The prior summarizes all information the researcher has on
the parameter values prior to observing the data. Denoting the prior as
<span class="math">\(\pi(\psi)\)</span>, the likelihood function as
<span class="math">\(\mathcal{L}(Y_n \mid \psi)\)</span>, and the posterior as
<span class="math">\(\pi(\psi \mid Y_n)\)</span>, we have</p>
<div class="math">
\[\pi(\psi \mid Y_n) \propto \mathcal{L}(Y_n \mid \psi) \pi(\psi)\]</div>
<p>The posterior distribution is the quantity of interest; the difficulty of
working with it depends on the prior specified by the researcher and the
likelihood function entailed by the selected model. In specific cases (for
example the special case of &#8220;conjugate priors&#8221;) the analytic form of the
posterior distribution can be found and used for analysis directly. More often
the posterior is not available analytically so other methods must be used to
explore its properties.</p>
<p>Posterior simulation is a method available when a procedure exists to <em>sample</em>
from the posterior distribution even though the analytic form of the
distribution may not be known. Posterior simulation considers drawing samples
<span class="math">\(\psi_s, s=1 \dots S\)</span>. Under fairly weak conditions a law of large
numbers can be applied so that, given the <span class="math">\(S\)</span> samples, sample averages
can be used to approximate population quantities:</p>
<div class="math">
\[\frac{1}{S} \sum_{s=1}^S g(\psi_s) \to \int g(\psi) \pi(\psi \mid Y_n) d \psi = E_{\pi(\cdot \mid Y_n)} \left [ g(\psi) \right ]\]</div>
<p>For example, the posterior mean is often of interest and corresponds to
<span class="math">\(g(\psi) = \psi\)</span>. Histograms can be used to examine the shapes of the
marginal distributions of individual parameters.</p>
<p>It may seem that sampling from an unknown distribution is impossible, but MCMC
methods allow the <em>eventual</em> sampling from an unknown distribution by applying
an algorithm designed to ensure that the unknown distribution is an invariant
distribution of a Markov chain. The Markov chain is initialized with an
arbitrary value, and then a transition density, denoted
<span class="math">\(f(\psi_s \mid \psi_{s-1})\)</span>, is applied to draw subsequent values
conditional only on the previous value. The appropriate selection of the
transition densities can usually ensure that there exists some value
<span class="math">\(\hat s\)</span> such that every subsequently drawn sample
<span class="math">\(\psi_s, ~ s &gt; \hat s\)</span> is marginally distributed according to the unknown
distribution of interest. <a class="footnote-reference" href="#id6" id="id4">[1]</a>  The two methods discussed below differ
in the specification of the transition density.</p>
<div class="section" id="markov-chain-monte-carlo-algorithms">
<h2>Markov chain Monte Carlo algorithms<a class="headerlink" href="#markov-chain-monte-carlo-algorithms" title="Permalink to this headline">¶</a></h2>
<p><strong>Metropolis-Hastings algorithm</strong> <a class="footnote-reference" href="#id7" id="id5">[2]</a></p>
<p>The Metropolis-Hastings algorithm is a very general strategy for constructing
a Markov chain with the desired invariant distribution. The transition density
is specified in the following way:</p>
<ol class="arabic">
<li><p class="first">Given the current value of the chain, <span class="math">\(\psi_{s-1}\)</span>, a proposal value,
<span class="math">\(\psi^*\)</span>, is selected according to a proposal
<span class="math">\(q(\psi ; \psi_{s-1})\)</span> which is a fixed density function for a given
value <span class="math">\(\psi_{s-1}\)</span>.</p>
</li>
<li><p class="first">With probability <span class="math">\(\alpha(\psi_{s-1}, \psi^*)\)</span> (defined below) the
proposed value is accepted so that the next value of the chain is set to
<span class="math">\(\psi_s = \psi^*\)</span>; if it is not accepted, the chain remains in place
<span class="math">\(\psi_s = \psi_{s-1}\)</span>.</p>
<div class="math">
\[\alpha(\psi_{s-1}, \psi^*) = \min \left \{ \frac{\pi(\psi^* \mid Y_n) q(\psi^* ; \psi_{s-1})}{\pi(\psi_{s-1} \mid Y_n) q(\psi_{s-1} ; \psi^*)}, 1 \right \}\]</div>
</li>
</ol>
<p>Practically speaking, the important component of this algorithm is that only
the ratio of posterior quantities is required. Recalling from above that the
posterior is proportional to the likelihood and the prior we can rewrite the
probability of acceptance as:</p>
<div class="math">
\[\alpha(\psi_{s-1}, \psi^*) = \min \left \{ \frac{\mathcal{L}(Y_n \mid \psi^*) \pi(\psi^*) q(\psi^* ; \psi_{s-1})}{\mathcal{L}(Y_n \mid \psi_{s-1}) \pi(\psi_{s-1}) q(\psi_{s-1} ; \psi^*)}, 1 \right \}\]</div>
<p>Given a particular specification for the prior and proposal distributions,
<em>this ratio can be computed</em> where the likelihood function is evaluated as a
byproduct of the Kalman filter iterations. In the special case that the
proposal distribution satistifes
<span class="math">\(q(\psi_{s-1} ; \psi^*) = q(\psi^* ; \psi_{s-1})\)</span> (as will be the case in
the examples below), we can again rewrite the probabilty of acceptance as</p>
<div class="math" id="equation-accept_prob">
<span class="eqno">(1)</span>\[\alpha(\psi_{s-1}, \psi^*) = \min \left \{ \frac{\mathcal{L}(Y_n \mid \psi^*) \pi(\psi^*)}{\mathcal{L}(Y_n \mid \psi_{s-1}) \pi(\psi_{s-1})}, 1 \right \}\]</div>
<p>One convenient choice of proposal distrubtion that allows this is the so-called
random walk proposal with Gaussian increment, defined such that</p>
<div class="math" id="equation-rw_proposal">
<span class="eqno">(2)</span>\[\psi^* = \psi_{s-1} + \epsilon_s, \qquad \epsilon_s \sim N(0, \Sigma_\epsilon)\]</div>
<p>Notice that to use this proposal distribution, we must set the variance
<span class="math">\(\Sigma_\epsilon\)</span>. This is often calibrated to achieve some target
acceptance rate (ratio of accepted to rejected draws); see the references above
for more details.</p>
<p><strong>Gibbs sampling algorithm</strong></p>
<p>Suppose that we can block the parameter vector into <span class="math">\(K\)</span> subvectors, so
that <span class="math">\(\psi = \{\psi^{(1)}, \psi^{(2)}, \dots, \psi^{(K)}\}\)</span>, and further
suppose that all <em>conditional</em> posterior distributions of the form
<span class="math">\(\pi(\psi^{(k)} \mid \psi^{(-k)}, Y_n), ~ k=1, \dots, K\)</span> can be sampled
from. Then the transition density moving from <span class="math">\(\psi_{s-1}\)</span> to
<span class="math">\(\psi_s\)</span> can be defined as follows:</p>
<ol class="arabic simple">
<li>Given the current value of the chain <span class="math">\(\psi_{s-1}\)</span>, sample
<span class="math">\(\psi_{s}^{(1)}\)</span> according to the density
<span class="math">\(\pi(\psi^{(1)} \mid \psi_{s-1}^{(-1)}, Y_n)\)</span>.</li>
<li>Sample <span class="math">\(\psi_{s}^{(2)}\)</span> according to the density
<span class="math">\(\pi(\psi^{(1)} \mid \psi_{s-1}^{(-1,2)}, \psi_{s}^{(1)}, Y_n)\)</span></li>
<li>[repeat for <span class="math">\(k=3, \dots, K\)</span>]</li>
<li>Then <span class="math">\(\psi_s = \{ \psi_s^{(1)}, \psi_s^{(2)}, \dots, \psi_s^{(K)} \}\)</span></li>
</ol>
<p>In the case of state space models, we can augment the parameter vector to
include the unobserved states. Notice then that the conditional posterior
distribution for the states is exactly the distribution from which the
simulation smoother produces simulated states; i.e. <span class="math">\(\tilde \alpha\)</span> is
drawn according to <span class="math">\(\pi(\alpha \mid \psi, Y_n)\)</span>.</p>
<p>The conditional distributions for the parameter vector must be identified on a
case-by-case basis. However, notice that the conditional posterior distribution
conditions on the unobserved states, so that in many cases the conditional
distributions follow from well known econometric problems. For example, if the
observation covariance matrix is diagonal, the parameters in the observation
equation can be viewed as equation-by-equation OLS.</p>
<p><strong>Metropolis-within-Gibbs sampling algorithm</strong></p>
<p>In the case that the parameter vector can be blocked as above but some of the
conditional posterior distributions cannot be directly sampled from, a hybrid
MCMC approach can be taken. The Gibbs sampling algorithm is used as defined
above, except that for any block <span class="math">\(k\)</span> such that the conditional posterior
cannot be sampled from, the Metropolis-Hastings algorithm is applied for that
block (i.e. a proposal is generated for that block of parameters and it is
accepted with the probability defined above).</p>
<table class="docutils footnote" frame="void" id="id6" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id4">[1]</a></td><td>Of course the value <span class="math">\(\hat s\)</span> is unknown and can in some cases be
quite large, although statistical tests do exist that can explore this
issue.</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="id7" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id5">[2]</a></td><td>This discussion is somewhat loose; see <a class="reference internal" href="9-references.html#tierney-markov-1994" id="id8">[30]</a> and
<a class="reference internal" href="9-references.html#chib-understanding-1995" id="id9">[6]</a> for careful treatments.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="implementing-metropolis-hastings-the-local-level-model">
<h2>Implementing Metropolis-Hastings: the local level model<a class="headerlink" href="#implementing-metropolis-hastings-the-local-level-model" title="Permalink to this headline">¶</a></h2>
<p>In this section we describe implementing the Metropolis-Hastings algorithm to
estimation unknown parameters of a state space model. First, it is illuminating
to consider a direct approach where all code is explicit; second, we consider
using the another Python library to streamline the estimation process.</p>
<p>Recalling the Metropolis-Hastings algorithm, above, in order to proceed we will
need to evaluate the likelihood and the prior and specify a proposal
distribution. The likelihood will be evaluated using the Kalman filter via the
<code class="docutils literal"><span class="pre">loglike</span></code> method introduced earlier. The local level, as written above, has
two variance parameters <span class="math">\(\sigma_\varepsilon^2\)</span> and <span class="math">\(\sigma_\eta^2\)</span>.
In practice we will sample the standard deviations <span class="math">\(\sigma_\varepsilon\)</span>
and <span class="math">\(\sigma_\eta\)</span>. These parameters are chosen to have independent
inverse Gamma priors, with the shape and scale parameters set as in
<a class="reference internal" href="#table-llevel-priors"><span>Table 5</span></a>. <a class="footnote-reference" href="#id11" id="id10">[3]</a> We will use the random walk proposal,
which simply requires drawing a value from a multivariate normal distribution
each iteration. We set the variance of the random walk innovation to be the
identity matrix times ten. The prior distributions can be evaluated and variates
drawn from the multivariate normal using the Python package SciPy.</p>
<table border="1" class="docutils" id="id22">
<span id="table-llevel-priors"></span><caption><span class="caption-number">Table 5 </span><span class="caption-text">Priors for the local level model applied to Nile data.</span><a class="headerlink" href="#id22" title="Permalink to this table">¶</a></caption>
<colgroup>
<col width="29%" />
<col width="22%" />
<col width="8%" />
<col width="8%" />
<col width="14%" />
<col width="18%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Parameter</th>
<th class="head">Prior distribution</th>
<th class="head">Shape</th>
<th class="head">Scale</th>
<th class="head">Prior mean</th>
<th class="head">Prior variance</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td><span class="math">\(\sigma_\varepsilon\)</span></td>
<td>Inverse-gamma</td>
<td>3</td>
<td>300</td>
<td>150</td>
<td>22,500</td>
</tr>
<tr class="row-odd"><td><span class="math">\(\sigma_\eta\)</span></td>
<td>Inverse-gamma</td>
<td>3</td>
<td>120</td>
<td>60</td>
<td>3,600</td>
</tr>
</tbody>
</table>
<p>For each iteration, the acceptance probability can be calculated from the above
elements, and the decision to accept or reject can be made by comparing the
acceptance probability to a random variate from a standard uniform
distribution.</p>
<table class="docutils footnote" frame="void" id="id11" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id10">[3]</a></td><td><p class="first">To be clear, since there are multiple ways to parameterize the
inverse-gamma distribution, with <span class="math">\(x \sim \text{IG}(\alpha, \beta)\)</span>
the density we consider is</p>
<div class="last math">
\[p(x) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{-\alpha - 1} e^{-\frac{\beta}{x}}\]</div>
</td></tr>
</tbody>
</table>
<div class="section" id="direct-approach">
<h3>Direct approach<a class="headerlink" href="#direct-approach" title="Permalink to this headline">¶</a></h3>
<p>Given the existence of the local level class (<code class="docutils literal"><span class="pre">MLELocalLevel</span></code>) for
calculating the loglikelihood, the code for performing MCMC exercise is
relatively simple. First, we initialize the priors and the proposal
distribution:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span><span class="p">,</span> <span class="n">invgamma</span><span class="p">,</span> <span class="n">uniform</span>

<span class="c"># Create the model for likelihood evaluation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLELocalLevel</span><span class="p">(</span><span class="n">nile</span><span class="p">)</span>

<span class="c"># Specify priors</span>
<span class="n">prior_obs</span> <span class="o">=</span> <span class="n">invgamma</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">prior_level</span> <span class="o">=</span> <span class="n">invgamma</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>

<span class="c"># Specify the random walk proposal</span>
<span class="n">rw_proposal</span> <span class="o">=</span> <span class="n">multivariate_normal</span><span class="p">(</span><span class="n">cov</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">*</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<p>Next, we perform 10,000 Metropolis-Hastings iterations as follows. The
resultant histograms and traces in terms of the variances, as well as a plot of
the acceptance ratio over the iterations, are given in
<a class="reference internal" href="#figure-5-llevel-posteriors"><span>Fig. 11</span></a>. <a class="footnote-reference" href="#id13" id="id12">[4]</a></p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># Create storage arrays for the traces</span>
<span class="n">n_iterations</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">trace</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_iterations</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">trace_accepts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">)</span>
<span class="n">trace</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">120</span><span class="p">,</span> <span class="mi">30</span><span class="p">]</span>  <span class="c"># Initial values</span>

<span class="c"># Iterations</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_iterations</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">proposed</span> <span class="o">=</span> <span class="n">trace</span><span class="p">[</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">rw_proposal</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>

    <span class="n">acceptance_probability</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">loglike</span><span class="p">(</span><span class="n">proposed</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">model</span><span class="o">.</span><span class="n">loglike</span><span class="p">(</span><span class="n">trace</span><span class="p">[</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span>
        <span class="n">prior_obs</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">proposed</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">+</span> <span class="n">prior_level</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">proposed</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">-</span>
        <span class="n">prior_obs</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">trace</span><span class="p">[</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">-</span> <span class="n">prior_level</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">trace</span><span class="p">[</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>

    <span class="k">if</span> <span class="n">acceptance_probability</span> <span class="o">&gt;</span> <span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">():</span>
        <span class="n">trace</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">proposed</span>
        <span class="n">trace_accepts</span><span class="p">[</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">trace</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">=</span> <span class="n">trace</span><span class="p">[</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
<div class="figure" id="id23">
<span id="figure-5-llevel-posteriors"></span><img alt="../../_images/fig_5-llevel-posteriors.png" src="../../_images/fig_5-llevel-posteriors.png" />
<p class="caption"><span class="caption-number">Fig. 11 </span><span class="caption-text">Output from Metropolis-Hastings posterior simulation on Nile data.</span></p>
</div>
<table class="docutils footnote" frame="void" id="id13" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id12">[4]</a></td><td>The output figures are ultimately based on 900 simulated values for each
parameter. Of the 10,000 simulations performed, the first 1,000 were
eliminated as the burn-in period, and the remaining 9,000 were thinned
by only taking each 10th sample, to reduce the effects of autocorrelated
draws.</td></tr>
</tbody>
</table>
</div>
<div class="section" id="integration-with-pymc">
<h3>Integration with PyMC<a class="headerlink" href="#integration-with-pymc" title="Permalink to this headline">¶</a></h3>
<p>This can also be simply estimated by taking advantage of the PyMC library
(<a class="reference internal" href="9-references.html#patil-pymc-2010" id="id14">[25]</a>). A full discussion of the features and
use of this library is beyond the scope of this paper and instead we only
introduce the features we need for estimation of this model. A similar approach
would handle most state space models, and the PyMC documentation can be
consulted for more advanced usage, including sophisticated sampling techniques
such as slice sampling and No-U-Turn sampling.</p>
<p>As above, we need to create objects representing the selected priors and an
object representing the likelihood function. The former are referred to by
PyMC as &#8220;stochastic&#8221; elements, and the latter as a &#8220;data&#8221; element (which is
a stochastic element that has already been &#8220;observed&#8221;, and so is not sampled
from). The priors and likelihood function using the <code class="docutils literal"><span class="pre">MLELocalLevel</span></code> class
defined above can be implemented with PyMC in the following way:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">pymc</span> <span class="kn">as</span> <span class="nn">mc</span>

<span class="c"># Priors as &quot;stochastic&quot; elements</span>
<span class="n">prior_obs</span> <span class="o">=</span> <span class="n">mc</span><span class="o">.</span><span class="n">InverseGamma</span><span class="p">(</span><span class="s">&#39;obs&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="n">prior_level</span> <span class="o">=</span> <span class="n">mc</span><span class="o">.</span><span class="n">InverseGamma</span><span class="p">(</span><span class="s">&#39;level&#39;</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">120</span><span class="p">)</span>

<span class="c"># Create the model for likelihood evaluation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MLELocalLevel</span><span class="p">(</span><span class="n">nile</span><span class="p">)</span>

<span class="c"># Create the &quot;data&quot; component (stochastic and observed)</span>
<span class="nd">@mc.stochastic</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dp</span><span class="o">.</span><span class="n">ssm</span><span class="o">.</span><span class="n">MLEModel</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">loglikelihood</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">obs_std</span><span class="o">=</span><span class="n">prior_obs</span><span class="p">,</span> <span class="n">level_std</span><span class="o">=</span><span class="n">prior_level</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">value</span><span class="o">.</span><span class="n">loglike</span><span class="p">([</span><span class="n">obs_std</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">level_std</span><span class="o">**</span><span class="mi">2</span><span class="p">])</span>
</pre></div>
</div>
<p>We do not need to explicitly specify the proposal; PyMC uses an adaptive
proposal by default. Instead, we simply need to create a &#8220;model&#8221;, which unifies
the priors and likelihood, and a &#8220;sampler&#8221;. The sampler is an object used to
perform the simulations and return the trace objects. The resultant histograms
and traces in terms of the variances from 10,000 iterations are given in
<a class="reference internal" href="#figure-5-pymc-posteriors"><span>Fig. 12</span></a>. <a class="footnote-reference" href="#id16" id="id15">[5]</a></p>
<div class="highlight-python"><div class="highlight"><pre><span class="c"># Create the PyMC model</span>
<span class="n">pymc_model</span> <span class="o">=</span> <span class="n">mc</span><span class="o">.</span><span class="n">Model</span><span class="p">((</span><span class="n">prior_obs</span><span class="p">,</span> <span class="n">prior_level</span><span class="p">,</span> <span class="n">loglikelihood</span><span class="p">))</span>

<span class="c"># Create a PyMC sample and perform sampling</span>
<span class="n">sampler</span> <span class="o">=</span> <span class="n">mc</span><span class="o">.</span><span class="n">MCMC</span><span class="p">(</span><span class="n">pymc_model</span><span class="p">)</span>
<span class="n">sampler</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="nb">iter</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">burn</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">thin</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
<div class="figure" id="id24">
<span id="figure-5-pymc-posteriors"></span><img alt="../../_images/fig_5-pymc-posteriors.png" src="../../_images/fig_5-pymc-posteriors.png" />
<p class="caption"><span class="caption-number">Fig. 12 </span><span class="caption-text">Output from Metropolis-Hastings posterior simulation on Nile data, using
the PyMC library.</span></p>
</div>
<table class="docutils footnote" frame="void" id="id16" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id15">[5]</a></td><td>The acceptance ratio is not provided by PyMC when the adaptive proposal
is used.</td></tr>
</tbody>
</table>
</div>
</div>
<div class="section" id="implementing-gibbs-sampling-the-arma-1-1-model">
<h2>Implementing Gibbs sampling: the ARMA(1,1) model<a class="headerlink" href="#implementing-gibbs-sampling-the-arma-1-1-model" title="Permalink to this headline">¶</a></h2>
<p>In this section we describe implementing the Gibbs sampling algorithm to
estimation unknown parameters of a state space model. Since PyMC only has
preliminary support for Gibbs sampling, only the direct approach is presented
here. The Metropolis-within-Gibbs approach is used to demonstrate both how to
apply Gibbs sampling and how to apply a hybrid approach.</p>
<p>Recalling the Gibbs sampling algorithm, above, in order to proceed we need to
block the parameters and the unobserved states such the the conditional
distributions can be found. We will choose four blocks, so that the unobserved
states are in the first block, the moving average coefficient is in the second
block, and the two autoregressive coefficients and variance are in the last
block. In notation, this means that
<span class="math">\(\psi = \{ \psi^{(1)}, \psi^{(2)}, \psi^{(3)}, \psi^{(4)} \}\)</span> where
<span class="math">\(\psi^{(1)} = \alpha\)</span>, <span class="math">\(\psi^{(2)} = \phi\)</span>,
<span class="math">\(\psi^{(4)} = \sigma^2\)</span>, and <span class="math">\(\psi^{(4)} = \theta\)</span>,.
We will apply Gibbs steps for the first, third, and fourth blocks, and a
Metropolis step for the second block.</p>
<p>We select priors for the parameters so that the conditional posterior
distributions that we require can be constructed. For the autogressive
coefficients, we select a multivariate normal distribution - conditional on the
variance - with an identity covariance matrix restricted to the space such that
the corresponding lag polynomial is invertible. To be precise, the prior
is <span class="math">\(\phi \mid \sigma^2 \sim N(0, I)\)</span>.</p>
<p>For the variance, we select an inverse-gamma distribution - conditional on the
autoregressive coefficients - with the shape and scale parameters both set to
two. To be precise, the prior is
<span class="math">\(\sigma^2 \mid \phi \sim IG(3, 3)\)</span>. These choices will be
convenient due to their status as conjugate priors for the linear regression
model; they will lead to known conditional posterior distributions.</p>
<p>Finally, the prior for the moving-average coefficient is specified to be
uniform over the interval <span class="math">\((-1, 1)\)</span>, so that
<span class="math">\(\theta \sim \text{unif}(-1, 1)\)</span>. Notice that the prior density for all
values in the range is equal, and so the acceptance probability is either zero,
in the case that the proposed value is outside the range, or else simplifies to
the ratio of the likelihoods because the prior values cancel out. We will use a
random walk proposal with standard normal increment.</p>
<p>Now, conditional on the model parameters, a draw of <span class="math">\(\psi^{(1)}\)</span> can be
taken by applying the simulation smoother, as shown in previous sections.
Next, notice that, given the values of the states, the first row of the
transition equation in <a href="#equation-arma11">(?)</a> is simply a linear regression:</p>
<div class="math">
\[\alpha_{1,t+1} = \phi \alpha_{1,t} + \varepsilon_{t+1}\]</div>
<p>Stacking into matrix form yields <span class="math">\(Z = X \phi + \varepsilon\)</span>.
A standard result applying conjugate priors to the linear regression model (see
for example <a class="reference internal" href="9-references.html#kim-state-space-1999" id="id17">[15]</a>) is that the conditional posterior
distribution for the coefficients is Gaussian and the conditional posterior
distribution for the variance is inverse-gamma. To be precise, given our choice
of prior hyperparameters here we have</p>
<div class="math">
\[\begin{split}\phi &amp; \mid \sigma^2, \alpha, Y_n \sim N \Big( (\sigma^2 I + X' X)^{-1} X' Z, (I + \sigma^{-2} X' X)^{-1} \Big ) \\
\sigma^2 &amp; \mid \phi, \alpha, Y_n \sim IG \Big (3 + n, 3 + (Z - X \phi)'(Z - X \phi) \Big )\end{split}\]</div>
<p>Making draws from these conditional posteriors can be implemented in the
following way:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">multivariate_normal</span><span class="p">,</span> <span class="n">invgamma</span>

<span class="k">def</span> <span class="nf">draw_posterior_phi</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">):</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">tmp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">sigma2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
    <span class="n">post_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">tmp</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Z</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>
    <span class="n">post_var</span> <span class="o">=</span> <span class="n">tmp</span> <span class="o">*</span> <span class="n">sigma2</span>

    <span class="k">return</span> <span class="n">multivariate_normal</span><span class="p">(</span><span class="n">post_mean</span><span class="p">,</span> <span class="n">post_var</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">draw_posterior_sigma2</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">phi</span><span class="p">):</span>
    <span class="n">resid</span> <span class="o">=</span> <span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">phi</span> <span class="o">*</span> <span class="n">states</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">post_shape</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">model</span><span class="o">.</span><span class="n">nobs</span>
    <span class="n">post_scale</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">resid</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">invgamma</span><span class="p">(</span><span class="n">post_shape</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">post_scale</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>
</pre></div>
</div>
<p>Implementing the hybrid method then consists of the following steps for each
iteration, given the previous value <span class="math">\(\psi_{s-1}\)</span> where <span class="math">\(\psi_0\)</span>
represents specified initial values.</p>
<ol class="arabic simple">
<li>Apply the simulation smoother to retrieve a draw of the unobserved states,
yielding <span class="math">\(\tilde \alpha = \psi_s^{(1)}\)</span>.</li>
<li>Draw a value for <span class="math">\(\phi = \psi_1^{(2)}\)</span>
from its conditional posterior distribution, conditioning on the states
drawn in step 1 and the parameters from the previous iteration.</li>
<li>Draw a value for <span class="math">\(\sigma^2 = \psi_s^{(3)}\)</span> from its conditional
posterior distribution, conditioning on the state states drawn in step 1 and
the autoregression coefficients drawn in step 2.</li>
<li>Propose a new value for <span class="math">\(\theta = \psi_s^{(4)}\)</span> using the random walk
proposal,  and calculate the acceptance probability using the <code class="docutils literal"><span class="pre">loglike</span></code> function.</li>
</ol>
<p>The implementation code is below, and the resultant histograms and traces from
10,000 iterations are given in <a class="reference internal" href="#figure-5-gibbs-posteriors"><span>Fig. 13</span></a>.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">norm</span><span class="p">,</span> <span class="n">uniform</span>
<span class="kn">from</span> <span class="nn">dismalpy.ssm.tools</span> <span class="kn">import</span> <span class="n">is_invertible</span>

<span class="c"># Create the model for likelihood evaluation and the simulation smoother</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ARMA11</span><span class="p">(</span><span class="n">inf</span><span class="p">)</span>
<span class="n">sim_smoother</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">simulation_smoother</span><span class="p">(</span><span class="n">simulation_output</span><span class="o">=</span><span class="n">dp</span><span class="o">.</span><span class="n">ssm</span><span class="o">.</span><span class="n">SIMULATION_STATE</span><span class="p">)</span>

<span class="c"># Create the random walk and comparison random variables</span>
<span class="n">rw_proposal</span> <span class="o">=</span> <span class="n">norm</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c"># Create storage arrays for the traces</span>
<span class="n">n_iterations</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">trace</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_iterations</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">trace_accepts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">)</span>
<span class="n">trace</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">1.</span><span class="p">]</span>  <span class="c"># Initial values</span>

<span class="c"># Iterations</span>
<span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_iterations</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="c"># 1. Gibbs step: draw the states using the simulation smoother</span>
    <span class="n">model</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">trace</span><span class="p">[</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">transformed</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">sim_smoother</span><span class="o">.</span><span class="n">simulate</span><span class="p">()</span>
    <span class="n">states</span> <span class="o">=</span> <span class="n">sim_smoother</span><span class="o">.</span><span class="n">simulated_state</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c"># 2. Gibbs step: draw the autoregressive parameters, and apply</span>
    <span class="c"># rejection sampling to ensure an invertible lag polynomial</span>
    <span class="n">phi</span> <span class="o">=</span> <span class="n">draw_posterior_phi</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">trace</span><span class="p">[</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
    <span class="k">while</span> <span class="ow">not</span> <span class="n">is_invertible</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="n">phi</span><span class="p">]):</span>
        <span class="n">phi</span> <span class="o">=</span> <span class="n">draw_posterior_phi</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">trace</span><span class="p">[</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">trace</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">phi</span>

    <span class="c"># 3. Gibbs step: draw the variance parameter</span>
    <span class="n">sigma2</span> <span class="o">=</span> <span class="n">draw_posterior_sigma2</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">phi</span><span class="p">)</span>
    <span class="n">trace</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">sigma2</span>

    <span class="c"># 4. Metropolis-step for the moving-average parameter</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">trace</span><span class="p">[</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">proposal</span> <span class="o">=</span> <span class="n">theta</span> <span class="o">+</span> <span class="n">rw_proposal</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">proposal</span> <span class="o">&gt;</span> <span class="o">-</span><span class="mi">1</span> <span class="ow">and</span> <span class="n">proposal</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">acceptance_probability</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">loglike</span><span class="p">([</span><span class="n">phi</span><span class="p">,</span> <span class="n">proposal</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">])</span> <span class="o">-</span>
            <span class="n">model</span><span class="o">.</span><span class="n">loglike</span><span class="p">([</span><span class="n">phi</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">sigma2</span><span class="p">]))</span>

        <span class="k">if</span> <span class="n">acceptance_probability</span> <span class="o">&gt;</span> <span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">():</span>
            <span class="n">theta</span> <span class="o">=</span> <span class="n">proposal</span>
            <span class="n">trace_accepts</span><span class="p">[</span><span class="n">s</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">trace</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">theta</span>
</pre></div>
</div>
<div class="figure" id="id25">
<span id="figure-5-gibbs-posteriors"></span><img alt="../../_images/fig_5-gibbs-posteriors.png" src="../../_images/fig_5-gibbs-posteriors.png" />
<p class="caption"><span class="caption-number">Fig. 13 </span><span class="caption-text">Output from Metropolis-within-Gibbs posterior simulation on US CPI
inflation data.</span></p>
</div>
</div>
<div class="section" id="implementing-gibbs-sampling-real-business-cycle-model">
<h2>Implementing Gibbs sampling: real business cycle model<a class="headerlink" href="#implementing-gibbs-sampling-real-business-cycle-model" title="Permalink to this headline">¶</a></h2>
<p>Finally, we can apply the same techniques as above to perform
Metropolis-within-Gibbs estimation of the real business cycle model parameters.
It is often difficult to estimate all of the parameters of the RBC model, or
indeed other structural models, with maximum likelihood. Above we only
estimated two of the six structural parameters. By choosing appropriately tight
priors it is often feasible to estimate more parameters; in this example we
estimate four of the six structural parameters: the discount rate, capital
share, and the two technology shock parameters. Of the two remaining
parameters, the disutility of labor only serves to pin down steady-state values
and so the model presented above is independent of its value (since it
considers data in in deviation-from-steady-state values), and the depreciation
rate is best calibrated when the observation datasets do not speak to
to depreciation (see for example <a class="reference internal" href="9-references.html#smets-shocks-2007" id="id18">[29]</a>).</p>
<p>For the Metropolis-within-Gibbs simulation, we consider 8 blocks. The first
three blocks are sampled using Gibbs steps, and are very similar to the
ARMA(1,1) example; the first block samples the unobserved states, and the
second and third blocks sample the two technology shock parameters. Noticing
that the second row of the transition equation is simply an autoregression,
conditional on the states, we can use the same approach we did above. Thus the
priors on these parameters are the Gaussian and inverse-gamma conjugate priors
and the unobserved states are sampled using the simulation smoother.</p>
<p>The remaining blocks apply Metropolis steps to sample the remaining five
parameters: the discount rate, capital share, and the three measurement
variances. The priors on these parameters are as in <a class="reference internal" href="9-references.html#smets-shocks-2007" id="id19">[29]</a>.
All priors are listed in <a class="reference internal" href="#table-rbc-priors"><span>Table 6</span></a>, along with statistics
describing the posterior draws.</p>
<table border="1" class="docutils" id="id26">
<span id="table-rbc-priors"></span><caption><span class="caption-number">Table 6 </span><span class="caption-text">Priors and posteriors for the real business cycle model.</span><a class="headerlink" href="#id26" title="Permalink to this table">¶</a></caption>
<colgroup>
<col width="32%" />
<col width="14%" />
<col width="6%" />
<col width="10%" />
<col width="8%" />
<col width="8%" />
<col width="10%" />
<col width="11%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">&nbsp;</th>
<th class="head" colspan="3">Prior distribution</th>
<th class="head" colspan="4">Posterior distribution</th>
</tr>
<tr class="row-even"><th class="head">&nbsp;</th>
<th class="head">Distribution</th>
<th class="head">Mean</th>
<th class="head">Std. Dev.</th>
<th class="head">Mode</th>
<th class="head">Mean</th>
<th class="head">5 percent</th>
<th class="head">95 percent</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-odd"><td>Discount rate <a class="footnote-reference" href="#id21" id="id20">[6]</a></td>
<td>Gamma</td>
<td>0.25</td>
<td>0.1</td>
<td>0.997</td>
<td>0.997</td>
<td>0.994</td>
<td>0.998</td>
</tr>
<tr class="row-even"><td>Capital share</td>
<td>Normal</td>
<td>0.3</td>
<td>0.01</td>
<td>0.325</td>
<td>0.325</td>
<td>0.308</td>
<td>0.341</td>
</tr>
<tr class="row-odd"><td>Technology shock persistence</td>
<td>Normal</td>
<td>0</td>
<td>1</td>
<td>0.672</td>
<td>0.637</td>
<td>-0.271</td>
<td>0.940</td>
</tr>
<tr class="row-even"><td>Technology shock variance</td>
<td>Inverse-gamma</td>
<td>0.01</td>
<td>1.414</td>
<td>8.65e-5</td>
<td>8.98e-5</td>
<td>7.67e-5</td>
<td>1.05e-4</td>
</tr>
<tr class="row-odd"><td>Output error standard deviation</td>
<td>Inverse-gamma</td>
<td>0.1</td>
<td>2</td>
<td>2.02e-5</td>
<td>2.29e-5</td>
<td>1.46e-5</td>
<td>3.34e-05</td>
</tr>
<tr class="row-even"><td>Labor error standard deviation</td>
<td>Inverse-gamma</td>
<td>0.1</td>
<td>2</td>
<td>3.06e-5</td>
<td>3.21e-5</td>
<td>2.25e-5</td>
<td>4.34e-05</td>
</tr>
<tr class="row-odd"><td>Consumption error standard deviation</td>
<td>Inverse-gamma</td>
<td>0.1</td>
<td>2</td>
<td>2.46e-5</td>
<td>2.57e-5</td>
<td>1.94e-5</td>
<td>3.28e-05</td>
</tr>
</tbody>
</table>
<p>Again, the code is slightly too long to display inline, so it can be found in
the appendix. We perform 100,000 draws and burn the first 10,000. Of the
remaining 90,000 draws, each tenth draw is saved, so that the results below are
ultimately based on 9,000 draws. Histograms of the four estimated structural
parameters are presented in <a class="reference internal" href="#figure-5-rbc-posteriors"><span>Fig. 14</span></a>.</p>
<div class="figure" id="id27">
<span id="figure-5-rbc-posteriors"></span><img alt="../../_images/fig_5-rbc-posteriors.png" src="../../_images/fig_5-rbc-posteriors.png" />
<p class="caption"><span class="caption-number">Fig. 14 </span><span class="caption-text">Output from Metropolis-within-Gibbs posterior simulation of the real business cycle.</span></p>
</div>
<p>As before, we may be interested in the implied impulse response functions and
the smoothed state values; here we calculate these be applying the Kalman
filter and smoother to the model based on the median parameter values.
<a class="reference internal" href="#figure-5-gibbs-irf"><span>Fig. 15</span></a> displays the impulse responses and
<a class="reference internal" href="#figure-5-gibbs-states"><span>Fig. 16</span></a> displays the smoothed states and confidence
intervals.</p>
<div class="figure" id="id28">
<span id="figure-5-gibbs-irf"></span><img alt="../../_images/fig_5-gibbs-irf.png" src="../../_images/fig_5-gibbs-irf.png" />
<p class="caption"><span class="caption-number">Fig. 15 </span><span class="caption-text">Impulse response functions corresponding to Metropolis-within-Gibbs
estimation of the real business cycle.</span></p>
</div>
<div class="figure" id="id29">
<span id="figure-5-gibbs-states"></span><img alt="../../_images/fig_5-gibbs-states.png" src="../../_images/fig_5-gibbs-states.png" />
<p class="caption"><span class="caption-number">Fig. 16 </span><span class="caption-text">Smoothed estimates of capital and the technology process from
Metropolis-within-Gibbs estimation of the real business cycle.</span></p>
</div>
<table class="docutils footnote" frame="void" id="id21" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#id20">[6]</a></td><td>If the discount rate is denoted <span class="math">\(\beta\)</span>, then the Gamma prior
actually applies to the transformation <span class="math">\(100 (\beta^{-1} - 1)\)</span>.</td></tr>
</tbody>
</table>
</div>
</div>


          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="6-out-of-the-box_models.html" class="btn btn-neutral float-right" title="Out-of-the-box models" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="4-maximum_likelihood_estimation.html" class="btn btn-neutral" title="Maximum Likelihood Estimation" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2014, Chad Fulton.
    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'0.2.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>